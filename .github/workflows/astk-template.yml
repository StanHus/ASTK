name: ASTK Agent Benchmark

on:
  workflow_call:
    inputs:
      agent-path:
        description: "Path to agent script"
        required: true
        type: string
      results-dir:
        description: "Directory to save results"
        required: false
        type: string
        default: "benchmark_results"
      python-version:
        description: "Python version to use"
        required: false
        type: string
        default: "3.11"
    secrets:
      OPENAI_API_KEY:
        description: "OpenAI API key for agent testing"
        required: false

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ inputs.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ inputs.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install ASTK from PyPI
        run: |
          pip install agent-sprint-testkit

      - name: Install project dependencies
        run: |
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          if [ -f setup.py ]; then pip install -e .; fi

      - name: Verify agent exists
        run: |
          if [ ! -f "${{ inputs.agent-path }}" ]; then
            echo "âŒ Agent file not found: ${{ inputs.agent-path }}"
            exit 1
          fi
          echo "âœ… Agent found: ${{ inputs.agent-path }}"

      - name: Test agent basic functionality
        run: |
          # Test that agent accepts command line arguments
          python "${{ inputs.agent-path }}" "Hello, this is a test question" || echo "âš ï¸ Agent may not accept CLI arguments"

      - name: Run ASTK Sophisticated Benchmarks
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python -c "
          import subprocess
          import sys
          from pathlib import Path

          # Use the simple_benchmark.py script directly
          result = subprocess.run([
            sys.executable, 'scripts/simple_benchmark.py',
            '${{ inputs.agent-path }}',
            '--results-dir', '${{ inputs.results-dir }}'
          ], capture_output=True, text=True)

          print('STDOUT:', result.stdout)
          if result.stderr:
            print('STDERR:', result.stderr)

          sys.exit(result.returncode)
          "

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v3
        with:
          name: astk-benchmark-results
          path: ${{ inputs.results-dir }}/
          retention-days: 30

      - name: Comment PR with Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Find latest results file
            const resultsDir = '${{ inputs.results-dir }}';

            if (!fs.existsSync(resultsDir)) {
              console.log('Results directory not found');
              return;
            }

            const files = fs.readdirSync(resultsDir);
            const jsonFiles = files.filter(f => f.endsWith('.json'));

            if (jsonFiles.length === 0) {
              console.log('No results files found');
              return;
            }

            const latestFile = jsonFiles.sort().pop();
            const resultsPath = path.join(resultsDir, latestFile);
            const results = JSON.parse(fs.readFileSync(resultsPath, 'utf8'));

            // Create comment body with sophisticated metrics
            const successRate = (results.success_rate * 100).toFixed(1);
            const complexityScore = ((results.complexity_score || 0) * 100).toFixed(1);
            const avgTime = results.average_scenario_duration.toFixed(2);
            const totalTime = results.total_duration_seconds.toFixed(2);

            // Get capability rating
            const complexity = results.complexity_score || 0;
            let capability = '';
            if (complexity >= 0.8) capability = 'ğŸŒŸ Exceptional AI';
            else if (complexity >= 0.6) capability = 'ğŸ”¥ Advanced AI'; 
            else if (complexity >= 0.4) capability = 'ğŸ’¡ Competent AI';
            else capability = 'ğŸ“š Developing AI';

            // Create difficulty breakdown
            const difficultyBreakdown = Object.entries(results.difficulty_breakdown || {})
              .map(([diff, data]) => {
                const emoji = {'intermediate': 'ğŸ“˜', 'advanced': 'ğŸ“™', 'expert': 'ğŸ“•'}[diff] || 'ğŸ“—';
                const rate = (data.success_rate * 100).toFixed(1);
                return `- ${emoji} **${diff.charAt(0).toUpperCase() + diff.slice(1)}**: ${rate}% (${data.scenarios})`;
              }).join('\n');

            // Create category breakdown  
            const categoryBreakdown = Object.entries(results.category_breakdown || {})
              .slice(0, 5) // Show top 5 categories
              .map(([cat, data]) => {
                const rate = (data.success_rate * 100).toFixed(1);
                return `- **${cat.replace(/_/g, ' ')}**: ${rate}% (${data.scenarios})`;
              }).join('\n');

            const comment = `## ğŸš€ ASTK Sophisticated Benchmark Results

            **Agent:** \`${{ inputs.agent-path }}\`

            ### ğŸ“Š Executive Summary
            - âœ… **Success Rate:** ${successRate}%
            - ğŸ§  **Complexity Score:** ${complexityScore}% (difficulty-weighted)
            - â±ï¸ **Average Response Time:** ${avgTime}s
            - ğŸ• **Total Time:** ${totalTime}s
            - ğŸ§ª **Scenarios:** ${results.total_scenarios || 12} sophisticated tests

            ### ğŸŒŸ AI Capability Assessment
            **${capability}** (Complexity Score: ${complexityScore}%)

            ### ğŸ“ Performance by Difficulty
            ${difficultyBreakdown || 'No difficulty breakdown available'}

            ### ğŸ·ï¸ Top Categories  
            ${categoryBreakdown || 'No category breakdown available'}

            ### ğŸ§ª Individual Scenario Results
            ${results.scenarios.map(s => {
              const duration = s.duration_seconds ? s.duration_seconds.toFixed(2) : 'N/A';
              return `- ${s.success ? 'âœ…' : 'âŒ'} **${s.scenario.replace(/_/g, ' ')}** (${duration}s)`;
            }).join('\n')}

            ${results.scenarios.some(s => !s.success) ? 
              '\nâš ï¸ Some sophisticated scenarios failed. This indicates areas for AI capability improvement.' : 
              '\nğŸ‰ All sophisticated scenarios passed! Excellent AI performance across multiple domains.'
            }

            ### ğŸ“ˆ Next Steps
            ${complexity < 0.6 ? '- Focus on improving reasoning and problem-solving capabilities\n- Consider advanced training on multi-step scenarios' : '- Consider testing with even more challenging expert-level scenarios\n- Explore specialized domain testing'}
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Optional: Benchmark comparison job for tracking improvements over time
  benchmark-comparison:
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'

    steps:
      - name: Download current results
        uses: actions/download-artifact@v3
        with:
          name: astk-benchmark-results
          path: current-results/

      - name: Compare with baseline (if available)
        run: |
          echo "ğŸ“Š Benchmark comparison would be implemented here"
          echo "This could compare current results with main branch results"
          echo "Current results saved in: current-results/"
